# -*- coding: utf-8 -*-
"""RFOREST CONGELACION BI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SDf8LDOYqGYAZBwf8I21XTIL1iTDg4T7
"""

!pip install xlsxwriter

!pip install --upgrade gspread pandas google-auth

"""**TABLA DE PREDICCIONES ACTUALIZADA DIRECTAMENTE A BI DESDE GOOGLE DRIVE**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
import joblib
from datetime import datetime
from google.colab import drive
import gspread
from google.colab import auth
from google.auth import default
from google.auth.exceptions import DefaultCredentialsError

# Montar Google Drive
drive.mount('/content/drive', force_remount=True)

# Cargar datos reales
data = pd.read_csv('TABLA CONGELACION.csv')

# Convertir la columna 'Local Start of Hour' a datetime
data['Local Start of Hour'] = pd.to_datetime(data['Local Start of Hour'], errors='coerce')

# Eliminar filas con valores NaT en 'Local Start of Hour'
data = data.dropna(subset=['Local Start of Hour'])

# Convertir otras columnas a valores numéricos (si es necesario)
for column in data.columns:
    if column != 'Local Start of Hour' and column != 'Desbalance Corriente CONGELACION':
        data[column] = pd.to_numeric(data[column], errors='coerce')

# Eliminar filas con valores no numéricos convertidos a NaN
data = data.dropna()

# Reasignar el índice y realizar el resampling
data = data.set_index('Local Start of Hour').resample('H').mean().dropna().reset_index()

# Variables objetivo
target_vars = [
    'Corriente congelacion F1',
    'Corriente congelacion F2',
    'Corriente congelacion F3',
    'Corriente resistencia deshielo'
]

# Normalizar los datos
scalers = {}
for var in target_vars:
    scaler = StandardScaler()
    data[var] = scaler.fit_transform(data[[var]])
    scalers[var] = scaler

# Crear características de fecha y hora adicionales
data['hour'] = data['Local Start of Hour'].dt.hour
data['day'] = data['Local Start of Hour'].dt.dayofyear
data['dayofweek'] = data['Local Start of Hour'].dt.dayofweek

# Preparar modelos y entrenar
models = {}
for var in target_vars:
    X = data[['hour', 'day', 'dayofweek']]
    y = data[var]
    model_rf = RandomForestRegressor(n_estimators=300, random_state=42)
    model_rf.fit(X, y)
    models[var] = model_rf
    joblib.dump(model_rf, f'modelo_rf_{var}.pkl')

# Guardar los escaladores
for var, scaler in scalers.items():
    joblib.dump(scaler, f'scaler_rf_{var}.pkl')

# Función para predecir variables horarias para un día específico usando Random Forest
def predict_rf_hourly(models, scalers, target_date, steps=24):
    target_date = pd.to_datetime(target_date)
    prediction_dates = pd.date_range(start=target_date, periods=steps, freq='H')
    prediction_hours = prediction_dates.hour
    prediction_days = prediction_dates.dayofyear
    prediction_dayofweek = prediction_dates.dayofweek
    X_pred = pd.DataFrame({
        'hour': prediction_hours,
        'day': prediction_days,
        'dayofweek': prediction_dayofweek
    })

    predictions = {}
    for var, model_rf in models.items():
        forecast = model_rf.predict(X_pred)
        forecast = scalers[var].inverse_transform(forecast.reshape(-1, 1)).flatten()
        predictions[var] = np.round(forecast, 2)  # Redondear predicciones a 2 decimales

    prediction_df = pd.DataFrame(predictions, index=prediction_dates)
    prediction_df.index.name = 'Time'
    return prediction_df

# Cargar los modelos y escaladores para hacer predicciones
models = {var: joblib.load(f'modelo_rf_{var}.pkl') for var in target_vars}
scalers = {var: joblib.load(f'scaler_rf_{var}.pkl') for var in target_vars}

# Fecha objetivo para la predicción
target_date = datetime.now().strftime('%Y-%m-%d')
#target_date = '2024-07-03'

# Realizar la predicción
predictions_df = predict_rf_hourly(models, scalers, target_date)

# Calcular el desbalance de corriente
predictions_df['PromedioFases'] = (predictions_df['Corriente congelacion F1'] + predictions_df['Corriente congelacion F2'] + predictions_df['Corriente congelacion F3']) / 3
predictions_df['Desbalance1'] = abs(predictions_df['Corriente congelacion F1'] - predictions_df['PromedioFases']) / predictions_df['PromedioFases'] * 100
predictions_df['Desbalance2'] = abs(predictions_df['Corriente congelacion F2'] - predictions_df['PromedioFases']) / predictions_df['PromedioFases'] * 100
predictions_df['Desbalance3'] = abs(predictions_df['Corriente congelacion F3'] - predictions_df['PromedioFases']) / predictions_df['PromedioFases'] * 100
predictions_df['Desbalance Corriente CONGELACION'] = predictions_df[['Desbalance1', 'Desbalance2', 'Desbalance3']].max(axis=1)

# Crear las columnas 'SENSORID' y 'SENSORNAME'
sensor_ids = {
    'Corriente congelacion F1': '5064C764-1FA5-49D8-9B8F-080D9EB38AB7',
    'Corriente congelacion F2': '8BF5C78C-6868-4C3C-82F9-DACF21790984',
    'Corriente congelacion F3': '3453DA27-AC58-485F-99F5-3E3A7A4CC3DC',
    'Corriente resistencia deshielo': 'FAABC39E-0CEF-451A-A5B7-D39E20C683D2',
    'Desbalance Corriente CONGELACION': '12345678-ABCD-EFGH-IJKL-MNOPQRSTUVWXYZ'
}

sensor_names = {
    'Corriente congelacion F1': 'FASE 1',
    'Corriente congelacion F2': 'FASE 2',
    'Corriente congelacion F3': 'FASE 3',
    'Corriente resistencia deshielo': 'DESHIELO',
    'Desbalance Corriente CONGELACION': 'DESBALANCE'
}

# Crear un DataFrame final con la estructura deseada
final_predictions = pd.DataFrame(columns=['FECHA', 'SENSORID', 'SENSORNAME', 'PREDICCIONES'])

for var in target_vars + ['Desbalance Corriente CONGELACION']:
    temp_df = predictions_df[[var]].copy()
    temp_df['FECHA'] = temp_df.index
    temp_df['SENSORID'] = sensor_ids[var]
    temp_df['SENSORNAME'] = sensor_names[var]
    temp_df = temp_df.rename(columns={var: 'PREDICCIONES'})
    final_predictions = pd.concat([final_predictions, temp_df])

# Crear la columna 'Tiempo' con el formato adecuado
final_predictions['Tiempo'] = pd.to_datetime(final_predictions['FECHA']).dt.strftime('%I:%M:%S %p')

# Reordenar las columnas
final_predictions = final_predictions[['FECHA', 'Tiempo', 'SENSORID', 'SENSORNAME', 'PREDICCIONES']]

# Asegurar que las columnas están en el tipo correcto
final_predictions['FECHA'] = final_predictions['FECHA'].astype(str)
final_predictions['SENSORID'] = final_predictions['SENSORID'].astype(str)
final_predictions['SENSORNAME'] = final_predictions['SENSORNAME'].astype(str)
final_predictions['PREDICCIONES'] = final_predictions['PREDICCIONES'].astype(float)

# Autenticación con Google Sheets
auth.authenticate_user()
creds, _ = default()
client = gspread.authorize(creds)

# Abrir o crear un Google Sheet
sheet_name = "PREDICCIONES CONGELACION"
try:
    spreadsheet = client.open(sheet_name)
except gspread.SpreadsheetNotFound:
    spreadsheet = client.create(sheet_name)

worksheet = spreadsheet.sheet1

# Leer los datos existentes en la hoja
existing_data = worksheet.get_all_records()

# Convertir los datos existentes a un DataFrame
if existing_data:
    existing_df = pd.DataFrame(existing_data)
else:
    existing_df = pd.DataFrame(columns=['FECHA', 'Tiempo', 'SENSORID', 'SENSORNAME', 'PREDICCIONES'])

# Concatenar los datos existentes con las nuevas predicciones
final_predictions = pd.concat([existing_df, final_predictions], ignore_index=True)

# Eliminar duplicados si los hay
final_predictions = final_predictions.drop_duplicates()

# Limpiar la hoja antes de actualizar
worksheet.clear()

# Actualizar los datos en Google Sheets
worksheet.update([final_predictions.columns.values.tolist()] + final_predictions.values.tolist())

print("El archivo CSV ha sido cargado y actualizado en Google Sheets.")